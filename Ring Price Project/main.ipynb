{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image as OpenpyxlImage\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from modules.database import init_db, save_to_sql\n",
    "\n",
    "# Import your custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import modules.scraper as scraper\n",
    "import modules.extractor as extractor\n",
    "from config.model_urls import model_urls\n",
    "from config.config import LOGIN_EMAIL, LOGIN_PASSWORD, main_url\n",
    "\n",
    "try:\n",
    "    from config.config import LOGIN_EMAIL, LOGIN_PASSWORD, main_url\n",
    "except ImportError:\n",
    "    LOGIN_EMAIL = os.getenv('LOGIN_EMAIL')\n",
    "    LOGIN_PASSWORD = os.getenv('LOGIN_PASSWORD')\n",
    "    main_url = os.getenv('MAIN_URL', 'https://example.com/login')\n",
    "\n",
    "try:\n",
    "    from config.model_urls import model_urls\n",
    "    print(\" Successfully load URLs from local list\")\n",
    "except ImportError:\n",
    "    model_urls = [] \n",
    "    print(\"Warning: Cannot find model_urls.py. Please create a correspoding file in config/ or import urls manually.\")\n",
    "\n",
    "# --- 1. SETUP FIREFOX BROWSER ---\n",
    "def init_driver():\n",
    "    \"\"\"Initializes the Firefox WebDriver with anti-detection settings.\"\"\"\n",
    "    options = FirefoxOptions()\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.set_preference(\"general.useragent.override\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\")\n",
    "    options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "    \n",
    "    service = FirefoxService(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    return driver\n",
    "\n",
    "# --- 2. EXCEL SAVING LOGIC (Maintains Image support for the exam) ---\n",
    "def save_results_to_xlsx(all_details):\n",
    "    \"\"\"\n",
    "    Saves extracted data to Excel and embeds product images in the first column.\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Product Data\"\n",
    "\n",
    "    # Define headers\n",
    "    headers = ['Image', 'Metal', 'Refcode', 'Description', 'Price']\n",
    "    ws.append(headers)\n",
    "\n",
    "    for row_idx, item in enumerate(all_details, start=2):\n",
    "        # 1. Fill Text Data\n",
    "        ws.cell(row=row_idx, column=2, value=item.get('Metal'))\n",
    "        ws.cell(row=row_idx, column=3, value=item.get('Refcode'))\n",
    "        ws.cell(row=row_idx, column=4, value=item.get('Description'))\n",
    "        ws.cell(row=row_idx, column=5, value=item.get('Price'))\n",
    "\n",
    "        # 2. Handle Image Download and Insertion\n",
    "        img_url = item.get('Image URL')\n",
    "        if img_url:\n",
    "            try:\n",
    "                response = requests.get(img_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    img_data = BytesIO(response.content)\n",
    "                    img = OpenpyxlImage(img_data)\n",
    "                    \n",
    "                    # Resize image to fit cell (approx 80x80 pixels)\n",
    "                    img.width = 80\n",
    "                    img.height = 80\n",
    "                    \n",
    "                    # Anchoring image to Column A\n",
    "                    ws.add_image(img, f'A{row_idx}')\n",
    "                    # Set row height to accommodate image\n",
    "                    ws.row_dimensions[row_idx].height = 65 \n",
    "            except Exception as e:\n",
    "                print(f\"Could not download image for {item.get('Refcode')}: {e}\")\n",
    "\n",
    "    # Column Formatting\n",
    "    ws.column_dimensions['A'].width = 12\n",
    "    ws.column_dimensions['D'].width = 40 # Description usually long\n",
    "    \n",
    "    filename = os.path.join('data', 'raw', 'ring_details.xlsx')\n",
    "    wb.save(filename)\n",
    "    print(f\"Excel file '{filename}' generated successfully!\")\n",
    "\n",
    "# --- 3. MAIN EXECUTION FLOW ---\n",
    "def main():\n",
    "    if not LOGIN_EMAIL or not LOGIN_PASSWORD:\n",
    "        print(\"Error: Missing credentials!\")\n",
    "        print(\"Please set config/config.py or environment variables.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    if \"example.com\" in main_url:\n",
    "        print(\"Warning: Using placeholder URL. The scraper might not work as expected.\")\n",
    "\n",
    "    os.makedirs(os.path.join('data', 'raw'), exist_ok=True)\n",
    "    os.makedirs(os.path.join('data', 'processed'), exist_ok=True)\n",
    "    \n",
    "    driver = init_driver()\n",
    "    # Load URLs (Replace with your testing_urls or list)\n",
    "\n",
    "    driver = init_driver()\n",
    "    all_extracted_data = []\n",
    "    init_db()\n",
    "\n",
    "    try:\n",
    "        # Step A: Perform Login\n",
    "        print(\"Starting login process...\")\n",
    "        if scraper.perform_login(driver, LOGIN_EMAIL, LOGIN_PASSWORD, main_url):\n",
    "            \n",
    "            # Step B: Loop through URLs\n",
    "            for i, url in enumerate(model_urls):\n",
    "                print(f\"Processing ({i+1}/{len(model_urls)}): {url}\")\n",
    "                \n",
    "                # Use Scraper Module\n",
    "                html = scraper.fetch_page_source(driver, url)\n",
    "                \n",
    "                if html:\n",
    "                    # Use Extractor Module\n",
    "                    data = extractor.extract_ring_details(html)\n",
    "                    if data:\n",
    "                        all_extracted_data.extend(data)\n",
    "                        print(f\"Successfully extracted {len(data)} variations.\")\n",
    "                \n",
    "                time.sleep(1) # Small delay between pages\n",
    "\n",
    "        # Step C: Save all data to Excel\n",
    "        if all_extracted_data:\n",
    "            save_results_to_xlsx(all_extracted_data)\n",
    "            save_to_sql(all_extracted_data)\n",
    "        else:\n",
    "            print(\"No data was extracted.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed. Task finished.\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
